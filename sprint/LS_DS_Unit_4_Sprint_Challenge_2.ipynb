{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Chocolate Gummy Bears](#Q2)\n",
    "    - Perceptron\n",
    "    - Multilayer Perceptron\n",
    "4. [Keras MMP](#Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:**\n",
    "    - A neuron can be thought of as a function. It takes multiple inputs in, multiples them by weights, sums them, adds a bias, applies an \"activation function\", and returns that activated value.\n",
    "- **Input Layer:**\n",
    "    - An input layer is the first layer of a neural network. It what takes in the data.\n",
    "- **Hidden Layer:**\n",
    "    - Hidden layers are the layers in the middle of a network. They are \"hidden\" because we don't really ever see them or directly interact with them, and by the time the data has gotten there, it's been changed so much it's almost unrecognizable.\n",
    "- **Output Layer:**\n",
    "    - The output layer is the final layer. It is what returns that actual predictions.\n",
    "- **Activation:**\n",
    "    - In the context of of neural networks, a node's activation function is what determines how valuable that node is. It does this by taking the weighted sum of that node (inputs * weights + bias) and applies some usually non linear function to that value. For instance with the sigmoid function, it \"squishify's\" that value around 0.5. So as the returned value approaches negative infinity, the sigmoid value approaches 0, and as it approaches positive infinity, it approaches 1.\n",
    "- **Backpropagation:**\n",
    "    - Backpropagation is the idea of updating all the weights and biases for every layer based on how well or poorly that given layers nodes performed on the output, determined by some cost function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
    "\n",
    "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
    "\n",
    "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
    "\n",
    "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
    "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "candy = pd.read_csv('chocolate_gummy_bears.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chocolate</th>\n",
       "      <th>gummy</th>\n",
       "      <th>ate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chocolate  gummy  ate\n",
       "0          0      1    1\n",
       "1          1      0    1\n",
       "2          0      1    1\n",
       "3          0      0    0\n",
       "4          1      1    0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(candy.shape)\n",
    "candy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
    "\n",
    "Once you've trained your model, report your accuracy. You will not be able to achieve more than ~50% with the simple perceptron. Explain why you could not achieve a higher accuracy with the *simple perceptron* architecture, because it's possible to achieve ~95% accuracy on this dataset. Provide your answer in markdown (and *optional* data anlysis code) after your perceptron implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Start your candy perceptron here\n",
    "\n",
    "X = candy[['chocolate', 'gummy']].values\n",
    "y = candy['ate'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron(object):\n",
    "    \n",
    "    def __init__(self, niter = 10):\n",
    "        np.random.seed(42)\n",
    "        self.niter = niter\n",
    "        \n",
    "    def __sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        sx = self.__sigmoid(x)\n",
    "        return sx / (1 - sx)\n",
    "    \n",
    "    def __add_ones(self, X):\n",
    "        ones = np.ones([X.shape[0], 1])\n",
    "        X = np.hstack([ones, X])\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_ones = self.__add_ones(X)\n",
    "        self.weights = np.random.random((X_ones.shape[1], 1))\n",
    "        \n",
    "        for i in range(self.niter):\n",
    "            weighted_sum = np.dot(X_ones, self.weights)\n",
    "\n",
    "            activated_output = self.__sigmoid(weighted_sum)\n",
    "            \n",
    "            activated_derivative = self.__sigmoid_derivative(weighted_sum)\n",
    "\n",
    "            error = y - activated_output\n",
    "            error_squared = error ** 2\n",
    "            print(\"error\", np.mean(error_squared))\n",
    "\n",
    "            self.weights += np.dot(X_ones.T, error*activated_derivative)\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_ones = self.__add_ones(X)\n",
    "        pred = np.dot(X_ones, self.weights)\n",
    "        return (pred <= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 0.31216790443391745\n",
      "error 0.5\n",
      "error 0.5\n",
      "error 0.5\n",
      "error 0.5\n",
      "error 0.5\n",
      "error 0.5\n",
      "error 0.5\n",
      "error 0.5\n",
      "error 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spentaur/.local/share/virtualenvs/lambda-e5gwxZ0x/lib/python3.7/site-packages/ipykernel_launcher.py:10: RuntimeWarning: overflow encountered in exp\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "perc = Perceptron(niter=10)\n",
    "perc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-15274.22939304],\n",
       "       [-14784.6007116 ],\n",
       "       [-14786.23971837]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.scatterplot(x=\"chocolate\", y=\"gummy\", data=candy, hue=\"ate\", alpha = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single layer perceptron can't fit non linear data, it's essentially logistic regression at this point. As you can see by the graph, there's no possible line that doesn't inlcude both ate 0 and ate 1, meaning the best accuracy is 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
    "Your network must have one hidden layer.\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([\n",
    "    [1,3],\n",
    "    [2,3]\n",
    "]).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795\n",
    "    \n",
    "class RelU:\n",
    "    def forward(self, input):\n",
    "        \"\"\"Apply elementwise ReLU to [batch, input_units] matrix\"\"\"\n",
    "        return np.maximum(0,input)\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"Compute gradient of loss w.r.t. ReLU input\"\"\"\n",
    "        relu_grad = input > 0\n",
    "        return grad_output*relu_grad \n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, input_nodes, output_nodes, learning_rate, activation_function = 'relu'):\n",
    "        self.input_nodes = input_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = np.random.rand(\n",
    "            self.input_nodes,\n",
    "            self.output_nodes\n",
    "        ) * 0.01\n",
    "        self.bias = np.random.rand(\n",
    "            self.output_nodes\n",
    "        ) * 0.01\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # TODO dot or matmul?\n",
    "        return np.matmul(inputs, self.weights) + self.bias\n",
    "    \n",
    "    def backward(self,input,grad_output):\n",
    "        # compute d f / d x = d f / d dense * d dense / d x\n",
    "        # where d dense/ d x = weights transposed\n",
    "        grad_input = np.dot(grad_output,np.transpose(self.weights))\n",
    "\n",
    "        # compute gradient w.r.t. weights and biases\n",
    "        grad_weights = np.transpose(np.dot(np.transpose(grad_output),input))\n",
    "        grad_biases = np.sum(grad_output, axis = 0)\n",
    "        \n",
    "        # Here we perform a stochastic gradient descent step. \n",
    "        # Later on, you can try replacing that with something better.\n",
    "        self.weights = self.weights - self.learning_rate * grad_weights\n",
    "        self.bias = self.bias - self.learning_rate * grad_biases\n",
    "        return grad_input\n",
    "        \n",
    "    \n",
    "class NeuralNetwork:\n",
    "    # TODO better variable names\n",
    "    def __init__(self, layers, learning_rate = 0.1, epochs = 100):\n",
    "        self.learning_rate = 0.1\n",
    "        self.epochs = epochs\n",
    "        self.layers = []\n",
    "        for layer in layers:\n",
    "            self.layers.append(\n",
    "                layer\n",
    "            )\n",
    "         \n",
    "    def forward(self, inputs):\n",
    "        activations = []\n",
    "        input = inputs\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            active = layer.forward(input)\n",
    "            activations.append(active)\n",
    "            input = active\n",
    "            \n",
    "        assert len(activations) == len(self.layers)\n",
    "            \n",
    "        return activations\n",
    "            \n",
    "    def cost_function(self, logits, reference_answers):\n",
    "        \"\"\"Compute crossentropy from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
    "        logits_for_answers = logits[:,reference_answers]\n",
    "\n",
    "        xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
    "        \n",
    "        return xentropy\n",
    "\n",
    "    def grad_cost_function(self, logits, reference_answers):\n",
    "        \"\"\"Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
    "        ones_for_answers = np.zeros_like(logits)\n",
    "        ones_for_answers[:, reference_answers] = 1\n",
    "    \n",
    "    \n",
    "        softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "\n",
    "        return (- ones_for_answers + softmax) / logits.shape[0]\n",
    "    \n",
    "    \n",
    "    def train(self, inputs, outputs):\n",
    "        \"\"\"\n",
    "        Train your network on a given batch of X and y.\n",
    "        You first need to run forward to get all layer activations.\n",
    "        Then you can run layer.backward going from last to first layer.\n",
    "        After you called backward for all layers, all Dense layers have already made one gradient step.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get the layer activations\n",
    "        layer_activations = self.forward(inputs)\n",
    "        logits = layer_activations[-1]\n",
    "\n",
    "        # Compute the loss and the initial gradient\n",
    "        loss = self.cost_function(logits,outputs)\n",
    "        loss_grad = self.grad_cost_function(logits,outputs)\n",
    "\n",
    "        for idx, layer in enumerate(reversed(self.layers)):\n",
    "            loss_grad = layer.backward(layer_activations[len(self.layers) - idx - 2], loss_grad)\n",
    "    \n",
    "    def iterate_minibatches(self, inputs, targets, batchsize, shuffle=False):\n",
    "        assert len(inputs) == len(targets)\n",
    "        if shuffle:\n",
    "            indices = np.random.permutation(len(inputs))\n",
    "        for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "            if shuffle:\n",
    "                excerpt = indices[start_idx:start_idx + batchsize]\n",
    "            else:\n",
    "                excerpt = slice(start_idx, start_idx + batchsize)\n",
    "            yield inputs[excerpt], targets[excerpt]\n",
    "    \n",
    "    def fit(self, inputs, outputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs.reshape(-1)\n",
    "        for epoch in range(self.epochs):\n",
    "            for x_batch,y_batch in self.iterate_minibatches(self.inputs,self.outputs,batchsize=32,shuffle=True):\n",
    "                self.train(x_batch,y_batch)\n",
    "                \n",
    "            y_pred = (self.forward(self.inputs))[-1].argmax(axis=-1)\n",
    "            print(np.mean(y_pred == self.outputs))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Layer(X.shape[1], 3, learning_rate = 0.1),\n",
    "    RelU(),\n",
    "    Layer(3,5, learning_rate = 0.1),\n",
    "    RelU(),\n",
    "    Layer(5,2, learning_rate = 0.1)\n",
    "]\n",
    "\n",
    "nn = NeuralNetwork(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/spentaur/.local/share/virtualenvs/lambda-e5gwxZ0x/lib/python3.7/site-packages/ipykernel_launcher.py:78: RuntimeWarning: overflow encountered in exp\n",
      "/Users/spentaur/.local/share/virtualenvs/lambda-e5gwxZ0x/lib/python3.7/site-packages/ipykernel_launcher.py:88: RuntimeWarning: overflow encountered in exp\n",
      "/Users/spentaur/.local/share/virtualenvs/lambda-e5gwxZ0x/lib/python3.7/site-packages/ipykernel_launcher.py:88: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/spentaur/.local/share/virtualenvs/lambda-e5gwxZ0x/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in greater\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "nn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Couldn't get this, I don't understand the math..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Don't try candy gummy bears. They're disgusting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras MMP <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>134</td>\n",
       "      <td>409</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>221</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>164</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>125</td>\n",
       "      <td>273</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>247</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "246   56    0   0       134   409    0        0      150      1      1.9   \n",
       "77    59    1   1       140   221    0        1      164      1      0.0   \n",
       "33    54    1   2       125   273    0        0      152      0      0.5   \n",
       "251   43    1   0       132   247    1        0      143      1      0.1   \n",
       "301   57    1   0       130   131    0        1      115      1      1.2   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "246      1   2     3       0  \n",
       "77       2   0     2       1  \n",
       "33       0   1     2       1  \n",
       "251      1   4     3       0  \n",
       "301      1   1     3       0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df = df.sample(frac=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>134</td>\n",
       "      <td>409</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>221</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>164</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>125</td>\n",
       "      <td>273</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>247</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "246   56    0   0       134   409    0        0      150      1      1.9   \n",
       "77    59    1   1       140   221    0        1      164      1      0.0   \n",
       "33    54    1   2       125   273    0        0      152      0      0.5   \n",
       "251   43    1   0       132   247    1        0      143      1      0.1   \n",
       "301   57    1   0       130   131    0        1      115      1      1.2   \n",
       "\n",
       "     slope  ca  thal  \n",
       "246      1   2     3  \n",
       "77       2   0     2  \n",
       "33       0   1     2  \n",
       "251      1   4     3  \n",
       "301      1   1     3  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "print(X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = X_train.shape[1]\n",
    "epochs = 75\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(64, activation='relu', input_shape=(inputs,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 242 samples, validate on 61 samples\n",
      "Epoch 1/75\n",
      "242/242 [==============================] - 1s 3ms/sample - loss: 0.2275 - acc: 0.9711 - val_loss: 2.9663 - val_acc: 0.7213\n",
      "Epoch 2/75\n",
      "242/242 [==============================] - 0s 446us/sample - loss: 0.1912 - acc: 0.9876 - val_loss: 4.0887 - val_acc: 0.6885\n",
      "Epoch 3/75\n",
      "242/242 [==============================] - 0s 586us/sample - loss: 0.1909 - acc: 0.9876 - val_loss: 4.1069 - val_acc: 0.6885\n",
      "Epoch 4/75\n",
      "242/242 [==============================] - 0s 969us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8874 - val_acc: 0.7049\n",
      "Epoch 5/75\n",
      "242/242 [==============================] - 0s 732us/sample - loss: 0.1916 - acc: 0.9876 - val_loss: 4.3526 - val_acc: 0.6721\n",
      "Epoch 6/75\n",
      "242/242 [==============================] - 0s 1ms/sample - loss: 0.1916 - acc: 0.9876 - val_loss: 4.1139 - val_acc: 0.6885\n",
      "Epoch 7/75\n",
      "242/242 [==============================] - 0s 394us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8914 - val_acc: 0.7049\n",
      "Epoch 8/75\n",
      "242/242 [==============================] - 0s 1ms/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8885 - val_acc: 0.7049\n",
      "Epoch 9/75\n",
      "242/242 [==============================] - 0s 596us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 10/75\n",
      "242/242 [==============================] - 0s 470us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 11/75\n",
      "242/242 [==============================] - 0s 460us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 12/75\n",
      "242/242 [==============================] - 0s 669us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 13/75\n",
      "242/242 [==============================] - 0s 938us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 14/75\n",
      "242/242 [==============================] - 0s 373us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 15/75\n",
      "242/242 [==============================] - 0s 448us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 16/75\n",
      "242/242 [==============================] - 0s 546us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 17/75\n",
      "242/242 [==============================] - 0s 984us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 18/75\n",
      "242/242 [==============================] - 0s 450us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 19/75\n",
      "242/242 [==============================] - 0s 332us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 20/75\n",
      "242/242 [==============================] - 0s 525us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 21/75\n",
      "242/242 [==============================] - 0s 619us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 22/75\n",
      "242/242 [==============================] - 0s 750us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 23/75\n",
      "242/242 [==============================] - 0s 406us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 24/75\n",
      "242/242 [==============================] - 0s 414us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 25/75\n",
      "242/242 [==============================] - 0s 393us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 26/75\n",
      "242/242 [==============================] - 0s 385us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 27/75\n",
      "242/242 [==============================] - 0s 334us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 28/75\n",
      "242/242 [==============================] - 0s 436us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 29/75\n",
      "242/242 [==============================] - 0s 480us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 30/75\n",
      "242/242 [==============================] - 0s 642us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 31/75\n",
      "242/242 [==============================] - 0s 444us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 32/75\n",
      "242/242 [==============================] - 0s 477us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 33/75\n",
      "242/242 [==============================] - 0s 499us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 34/75\n",
      "242/242 [==============================] - 0s 664us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 35/75\n",
      "242/242 [==============================] - 0s 538us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 36/75\n",
      "242/242 [==============================] - 0s 1ms/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 37/75\n",
      "242/242 [==============================] - 0s 777us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 38/75\n",
      "242/242 [==============================] - 0s 603us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 39/75\n",
      "242/242 [==============================] - 0s 860us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 40/75\n",
      "242/242 [==============================] - 0s 890us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 41/75\n",
      "242/242 [==============================] - 0s 936us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 42/75\n",
      "242/242 [==============================] - 0s 720us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 43/75\n",
      "242/242 [==============================] - 0s 644us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 44/75\n",
      "242/242 [==============================] - 0s 551us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 45/75\n",
      "242/242 [==============================] - 0s 463us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 46/75\n",
      "242/242 [==============================] - 0s 484us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 47/75\n",
      "242/242 [==============================] - 0s 398us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 48/75\n",
      "242/242 [==============================] - 0s 574us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 49/75\n",
      "242/242 [==============================] - 0s 440us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 50/75\n",
      "242/242 [==============================] - 0s 504us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 51/75\n",
      "242/242 [==============================] - 0s 374us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 52/75\n",
      "242/242 [==============================] - 0s 477us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 53/75\n",
      "242/242 [==============================] - 0s 412us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 54/75\n",
      "242/242 [==============================] - 0s 477us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 55/75\n",
      "242/242 [==============================] - 0s 453us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 56/75\n",
      "242/242 [==============================] - 0s 405us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 57/75\n",
      "242/242 [==============================] - 0s 439us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 58/75\n",
      "242/242 [==============================] - 0s 527us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 59/75\n",
      "242/242 [==============================] - 0s 1ms/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 60/75\n",
      "242/242 [==============================] - 0s 900us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 61/75\n",
      "242/242 [==============================] - 0s 624us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 62/75\n",
      "242/242 [==============================] - 0s 859us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 63/75\n",
      "242/242 [==============================] - 0s 633us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 64/75\n",
      "242/242 [==============================] - 0s 427us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 65/75\n",
      "242/242 [==============================] - 0s 460us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 66/75\n",
      "242/242 [==============================] - 0s 443us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 67/75\n",
      "242/242 [==============================] - 0s 407us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 68/75\n",
      "242/242 [==============================] - 0s 433us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 69/75\n",
      "242/242 [==============================] - 0s 477us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 70/75\n",
      "242/242 [==============================] - 0s 545us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 71/75\n",
      "242/242 [==============================] - 0s 652us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 72/75\n",
      "242/242 [==============================] - 0s 438us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 73/75\n",
      "242/242 [==============================] - 0s 396us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 74/75\n",
      "242/242 [==============================] - 0s 637us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n",
      "Epoch 75/75\n",
      "242/242 [==============================] - 0s 644us/sample - loss: 0.1908 - acc: 0.9876 - val_loss: 3.8883 - val_acc: 0.7049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13be4cad0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, \n",
    "          validation_data=(x_test,y_test), \n",
    "          epochs=epochs, \n",
    "          batch_size=batch_size\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(25, activation='relu', input_shape=(inputs,)))\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'batch_size': list(range(15, 106, 15)),\n",
    "    'epochs': list(range(10, 101, 10)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 70 candidates, totalling 350 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   14.5s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   20.4s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   24.9s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   37.4s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   56.9s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=-1)]: Done 330 tasks      | elapsed: 10.6min\n",
      "[Parallel(n_jobs=-1)]: Done 350 out of 350 | elapsed: 11.6min finished\n",
      "/Users/spentaur/.local/share/virtualenvs/lambda-e5gwxZ0x/lib/python3.7/site-packages/sklearn/model_selection/_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5, \n",
    "    verbose=10,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_result = grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8471074375239286"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_score_"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "lambda",
   "language": "python",
   "name": "lambda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
