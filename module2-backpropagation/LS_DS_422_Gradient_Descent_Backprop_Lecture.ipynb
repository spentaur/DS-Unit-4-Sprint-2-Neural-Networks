{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 2, Module 2*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Backpropagation & Gradient Descent (Prepare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "* <a href=\"#p1\">Part 1</a>: Explain the intutition behind backproprogation\n",
    "* <a href=\"#p2\">Part 2</a>: Implement gradient descent + backpropagation on a feedforward neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Yesterday\n",
    "\n",
    "Yesterday, we learned about some of the principal components of Neural Networks: Neurons, Weights, Activation Functions, and layers (input, output, & hidden). Today, we will reinfornce our understanding of those components and introduce the mechanics of training a neural network. Feedforward neural networks, such as multi-layer perceptrons (MLPs), are almost always trained using some variation of gradient descent where the gradient has been calculated by backpropagation.\n",
    "\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/1600/1*_M4bZyuwaGby6KMiYVYXvg.jpeg\" width=\"400\"></center>\n",
    "\n",
    "- There are three kinds of layers: input, hidden, and output layers.\n",
    "- Each layer is made up of **n** individual neurons (aka activation units) which have a corresponding weight and bias.\n",
    "- Signal is passed from layer to layer through a network by:\n",
    " - Taking in inputs from the training data (or previous layer)\n",
    " - Multiplying each input by its corresponding weight (think arrow/connecting line)\n",
    " - Adding a bias to this weighted some of inputs and weights\n",
    " - Activating this weighted sum + bias by squishifying it with sigmoid or some other activation function. With a single perceptron with three inputs, calculating the output from the node is done like so:\n",
    "\\begin{align}\n",
    " y = sigmoid(\\sum(weight_{1}input_{1} + weight_{2}input_{2} + weight_{3}input_{3}) + bias)\n",
    "\\end{align}\n",
    " - this final activated value is the signal that gets passed onto the next layer of the network.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Neural Network: *Formal Summary*\n",
    "\n",
    "0. Pick a network architecture\n",
    "   - No. of input units = No. of features\n",
    "   - No. of output units = Number of Classes (or expected targets)\n",
    "   - Select the number of hidden layers and number of neurons within each hidden layer\n",
    "1. Randomly initialize weights\n",
    "2. Implement forward propagation to get $h_{\\theta}(x^{(i)})$ for any $x^{(i)}$\n",
    "3. Implement code to compute a cost function $J(\\theta)$\n",
    "4. Implement backpropagation to compute partial derivatives $\\frac{\\delta}{\\delta\\theta_{jk}^{l}}{J(\\theta)}$\n",
    "5. Use gradient descent (or other advanced optimizer) with backpropagation to minimize $J(\\theta)$ as a function of parameters $\\theta\\$\n",
    "6. Repeat steps 2 - 5 until cost function is 'minimized' or some other stopping criteria is met. One pass over steps 2 - 5 is called an iteration or epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Calculating *\"cost\"*, *\"loss\"* or *\"error\"*\n",
    "\n",
    "We've talked about how in order to evaluate a network's performance, the data is \"fed forward\" until predictions are obtained and then the \"loss\" or \"error\" for a given observation is ascertained by looking at what the network predicted for that observation and comparing it to what it *should* have predicted. \n",
    "\n",
    "The error for a given observation is calculated by taking the square of the difference between the predicted value and the actual value. \n",
    "\n",
    "We can summarize the overal quality of a network's predictions by finding the average error across all observations. This gives us the \"Mean Squared Error.\" which hopefully is a fairly familiar model evaluation metric by now. Graphing the MSE over each epoch (training cycle) is a common practice with Neural Networks. This is what you're seeing in the top right corner of the Tensorflow Playground website as the number of \"epochs\" climbs higher and higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an \"Epoch\"?\n",
    "\n",
    "An \"Epoch\" is one cycle of passing our data forward through the network, measuring error given our specified cost function, and then -via gradient descent- updating weights within our network to hopefully improve the quality of our predictions on the next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch vs Minibatch vs Stochastic Gradient Descent Epochs\n",
    "\n",
    "You may have heard these variations on the training process referenced in the 3Blue1Brown videos about backpropagation. \"Minibatch\" Gradient Descent means that instead of passing all of our data through the network for a given epoch (Batch GD), we just pass a randomized portion of our data through the network for each epoch. \n",
    "\n",
    "Stochastic Gradient Descent is when we make updates to our weights after forward propagating each individual training observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about Hyperparameters\n",
    "\n",
    "Neural Networks have many more hyperparameters than other machine learning algorithms which is part of what makes them a beast to train.\n",
    "\n",
    "1. You need more data to train them on. \n",
    "2. They're complex so they take longer to train. \n",
    "3. They have lots and lots of hyperparameters which we need to find the most optimal combination of, so we might end up training our model dozens or hundreds of times with different combinations of hyperparameters in order to try and squeeze out a few more tenths of a percent of accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aM4CK1IarId4",
    "toc-hr-collapsed": false
   },
   "source": [
    "# Backpropagation (Learn)\n",
    "<a id=\"p1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aM4CK1IarId4",
    "toc-hr-collapsed": false
   },
   "source": [
    "## Overview\n",
    "\n",
    "Backpropagation is short for [\"Backwards Propagation of errors\"](https://en.wikipedia.org/wiki/Backpropagation) and refers to a specific (rather calculus intensive) algorithm for how weights in a neural network are updated in reverse order at the end of each training epoch. Our purpose today is to demonstrate the backpropagation algorithm on a simple Feedforward Neural Network and in so doing help you get a grasp on the main process. If you want to understand all of the underlying calculus of how the gradients are calculated then you'll need to dive into it yourself, [3Blue1Brown's video is a great starting place](https://www.youtube.com/watch?v=tIeHLnjs5U8). I also highly recommend this Welch Labs series [Neural Networks Demystified](https://www.youtube.com/watch?v=bxe2T-V8XRs) if you want a rapid yet orderly walkthrough of the main intuitions and math behind the backpropagation algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Gradient?\n",
    "\n",
    "> In vector calculus, the gradient is a multi-variable generalization of the derivative. \n",
    "\n",
    "The gradients that we will deal with today will be vector representations of the derivative of the activation function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "In this section, we will again implement a multi-layer perceptron using numpy. We'll focus on using a __Feed Forward Neural Network__ to predict test scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dm2HPETcrgy6",
    "toc-hr-collapsed": true
   },
   "source": [
    "![231 Neural Network](https://cdn-images-1.medium.com/max/1600/1*IjY3wFF24sK9UhiOlf36Bw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4d4tzpwO6B47"
   },
   "source": [
    "### Generate some Fake Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ERyVgeO_IWyV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(812)\n",
    "\n",
    "# hours studying, hours sleep\n",
    "X = np.array(([2,9],\n",
    "              [1,5],\n",
    "              [3,6]), dtype=float)\n",
    "\n",
    "# Exam Scores\n",
    "y = np.array(([92],\n",
    "              [86],\n",
    "              [89]), dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cDeUBW6k4Ri4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Studying, Sleeping \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Test Score \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n"
     ]
    }
   ],
   "source": [
    "# Normalizing Data on feature \n",
    "# Neural Network would probably do this on its own, but it will help us converge on a solution faster\n",
    "X = X / np.amax(X, axis=0)\n",
    "y = y / 100\n",
    "\n",
    "print(\"Studying, Sleeping \\n\", X)\n",
    "print(\"Test Score \\n\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bgTf6vTS69Sw"
   },
   "source": [
    "### Neural Network Architecture\n",
    "Lets create a Neural_Network class to contain this functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RUI8VSR5zyBv"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Set up Architecture of Neural Network\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 3\n",
    "        self.outputNodes = 1\n",
    "\n",
    "        # Initial Weights\n",
    "        # 2x3 Matrix Array for the First Layer\n",
    "        self.weights1 = np.random.rand(self.inputs, self.hiddenNodes)\n",
    "       \n",
    "        # 3x1 Matrix Array for Hidden to Output\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gbyT_FJ88IlK"
   },
   "source": [
    "### Randomly Initialize Weights\n",
    "How many random weights do we need to initialize? \"Fully-connected Layers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IreIDe6P8H0H"
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 wieghts: \n",
      " [[0.5049808  0.60592761 0.45748719]\n",
      " [0.32659171 0.59345002 0.25569456]]\n",
      "Layer 2 wieghts: \n",
      " [[0.23870931]\n",
      " [0.95553049]\n",
      " [0.95889787]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Layer 1 wieghts: \\n\", nn.weights1)\n",
    "print(\"Layer 2 wieghts: \\n\", nn.weights2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hbxDhyjQ-RwS"
   },
   "source": [
    "### Implement Feedforward Functionality\n",
    "\n",
    "After this step our neural network should be able to generate an output even though it has not been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gGivpEk-VdP"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Set up Architecture of Neural Network\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 3\n",
    "        self.outputNodes = 1\n",
    "\n",
    "        # Initial Weights\n",
    "        # 2x3 Matrix Array for the First Layer\n",
    "        self.weights1 = np.random.rand(self.inputs, self.hiddenNodes)\n",
    "       \n",
    "        # 3x1 Matrix Array for Hidden to Output\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        aka \"predict\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # Weighted sum of inputs => hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        # Activations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Weight sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        # Final activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a1pxdfmDAaJg"
   },
   "source": [
    "### Make a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intput [0.66666667 1.        ]\n",
      "output [0.79105842]\n"
     ]
    }
   ],
   "source": [
    "# Try to make a prediction with our updated 'net\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "output = nn.feed_forward(X[0])\n",
    "print(\"intput\", X[0])\n",
    "print(\"output\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3V61yNmAB2T5"
   },
   "source": [
    "### Calculate Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12894158])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = y[0] - output\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.79105842]\n",
      " [0.7590318 ]\n",
      " [0.79893077]]\n",
      "[[0.12894158]\n",
      " [0.1009682 ]\n",
      " [0.09106923]]\n"
     ]
    }
   ],
   "source": [
    "output_all = nn.feed_forward(X)\n",
    "error_all = y - output_all\n",
    "print(output_all)\n",
    "print(error_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "26wgCLU0TLvy"
   },
   "source": [
    "Why is my error so big?\n",
    "\n",
    "My error is so big because my prediction is low.\n",
    "\n",
    "Why are my prediction low?\n",
    "\n",
    "Because either:\n",
    "\n",
    "  1) Second layer **weights** are low\n",
    "  \n",
    "  (or)\n",
    "  \n",
    "  2) Activations coming from the first layer are low\n",
    "  \n",
    "How are activations from the first layer determined? \n",
    "\n",
    "  1) By inputs - fixed\n",
    "  \n",
    "  2) by **weights** - variable\n",
    "  \n",
    "The only thing that I have control over throughout this process in order to increase the value of my final predictions is to either increase weights in layer 2 or increase weights in layer 1. \n",
    "\n",
    "Imagine that you could only change your weights by a fixed amount. Say you have .3 and you have to split that up and disperse it over your weights so as to increase your predictions as much as possible. (This isn't actually what happens, but it will help us identify which weights we would benefit the most from moving.)\n",
    "\n",
    "I need to increase weights of my model somewhere, I'll get the biggest bang for my buck if I increase weights in places where I'm already seeing high activation values -because they end up getting multiplied together before being passed to the sigmoid function. \n",
    "\n",
    "> \"Neurons that fire together, wire together\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j_eyzItYIxgm"
   },
   "source": [
    "### Implement Backpropagation \n",
    "\n",
    "> *Assigning blame for bad predictions and delivering justice - repeatedly and a little bit at a time*\n",
    "\n",
    "What in our model could be causing our predictions to suck so bad? \n",
    "\n",
    "Well, we know that our inputs (X) and outputs (y) are correct, if they weren't then we would have bigger problems than understanding backpropagation.\n",
    "\n",
    "We also know that our activation function (sigmoid) is working correctly. It can't be blamed because it just does whatever we tell it to and transforms the data in a known way.\n",
    "\n",
    "So what are the potential culprits for these terrible predictions? The **weights** of our model. Here's the problem though. I have weights that exist in both layers of my model. How do I know if the weights in the first layer are to blame, or the second layer, or both? \n",
    "\n",
    "Lets investigate. And see if we can just eyeball what should be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights1\n",
      " [[0.73665387 0.81926386 0.88654768]\n",
      " [0.92386319 0.28768809 0.29744598]] \n",
      "---------\n",
      "hidden_sum\n",
      " [[1.41496577 0.833864   0.88847777]\n",
      " [0.75880862 0.43291467 0.46076366]\n",
      " [1.35256266 1.01105592 1.084845  ]] \n",
      "---------\n",
      "activated_hidden\n",
      " [[0.80454799 0.69717133 0.70857594]\n",
      " [0.68109502 0.60656945 0.61319532]\n",
      " [0.79454828 0.73322674 0.74740976]] \n",
      "---------\n",
      "weights2\n",
      " [[0.43328179]\n",
      " [0.56557272]\n",
      " [0.83042626]] \n",
      "---------\n",
      "activated_output\n",
      " [[0.79105842]\n",
      " [0.7590318 ]\n",
      " [0.79893077]] \n",
      "---------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attributes = ['weights1', 'hidden_sum', 'activated_hidden', 'weights2', 'activated_output']\n",
    "[print(i+'\\n', getattr(nn,i), '\\n'+'---'*3) for i in attributes if i[:2]!= '__'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16Ujj6vNYQyX"
   },
   "source": [
    "### Backpropagation by Hand (Not Recommended)\n",
    "\n",
    "Our model has 9 total weights (6 in the first layer, 3 in the last layer) that could be off.\n",
    "\n",
    "1) Calculate Error for a given each observation\n",
    "\n",
    "2) Does the error indicate that I'm overestimating or underestimating in my prediction?\n",
    "\n",
    "3) Look at final layer weights to get an idea for which weights are helping pass desireable signals and which are stifling desireable signals\n",
    "\n",
    "4) Also go to the previous layer and see what can be done to boost activations that are associated with helpful weights, and limit activations that are associated with unhelpful weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rPkfRI-iMvoV"
   },
   "outputs": [],
   "source": [
    "# We want activated that correspond to negative weights to be lower\n",
    "# And we want more higher activation for positivie weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "16Ujj6vNYQyX",
    "toc-hr-collapsed": true
   },
   "source": [
    "### Update Weights Based on Gradient\n",
    "\n",
    "Repeat steps 1-4 for every observation in a given batch, and then given the network's cost function, calculate its gradient using calculus and update weights associated with the (negative) gradient of the cost function. \n",
    "\n",
    "Remember that we have 9 weights in our network therefore the gradient that comes from our gradient descent calculation will be the vector that takes us in the most downward direction along some function in 9-dimensional hyperspace.\n",
    "\n",
    "\\begin{align}\n",
    "C(w1, w2, w3, w4, w5, w6, w7, w8, w9)\n",
    "\\end{align}\n",
    "\n",
    "You should also know that with neural networks it is common to have gradients that are not convex (like what we saw when we applied gradient descent to linear regression). Due to the high complexity of these models and their nonlinearity, it is common for gradient descent to get stuck in a local minimum, but there are ways to combat this:\n",
    "\n",
    "1) Stochastic Gradient Descent\n",
    "\n",
    "2) More advanced Gradient-Descent-based \"Optimizers\" - See Stretch Goals on assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want activations that correspond to negative weights to be lower\n",
    "# and activations that correspond to positive weights to be higher\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Set up Architecture of Neural Network\n",
    "        self.inputs = 2\n",
    "        self.hiddenNodes = 3\n",
    "        self.outputNodes = 1\n",
    "\n",
    "        # Initial Weights\n",
    "        # 2x3 Matrix Array for the First Layer\n",
    "        self.weights1 = np.random.rand(self.inputs, self.hiddenNodes)\n",
    "       \n",
    "        # 3x1 Matrix Array for Hidden to Output\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes)\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        aka \"predict\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # Weighted sum of inputs => hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        # Activations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Weight sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        # Final activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "        \n",
    "    def backward(self, X,y,o):\n",
    "        \"\"\"\n",
    "        Backward propagate through the network\n",
    "        \"\"\"\n",
    "        \n",
    "        # Error in Output\n",
    "        self.o_error = y - o\n",
    "        \n",
    "        # Apply Derivative of Sigmoid to error\n",
    "        # How far off are we in relation to the Sigmoid f(x) of the output\n",
    "        # ^- aka hidden => output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        \n",
    "        # z2 error\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        \n",
    "        # How much of that \"far off\" can explained by the input => hidden\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)\n",
    "        \n",
    "        # Adjustment to first set of weights (input => hidden)\n",
    "        self.weights1 += X.T.dot(self.z2_delta)\n",
    "        # Adjustment to second set of weights (hidden => output)\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta)\n",
    "        \n",
    "\n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X,y,o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "#### Let's look at the shape of the Gradient Componets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()\n",
    "\n",
    "nn.train(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Our Error Associated with Each Observation \n",
    "aka how wrong were we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09321167],\n",
       "       [0.05811254],\n",
       "       [0.05684366]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.o_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1st Gradient \n",
    "Simple interpretation - how much more sigmoid activation would have pushed us towards the right answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01334879],\n",
       "       [0.00923199],\n",
       "       [0.00790166]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.o_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2nd Error\n",
    "Justice hasn't been served yet - tho. We still have neurons to blame. Let's go back another layer. \n",
    "\n",
    "__Discussion:__ Why is this shape different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0129298 , 0.0093052 , 0.00994677],\n",
       "       [0.00894222, 0.00643546, 0.00687916],\n",
       "       [0.00765364, 0.00550811, 0.00588787]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.z2_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2nd Gradient\n",
    "For each observation, how much more sigmoid activation from this layer would have pushed us towards the right answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00310159, 0.00213589, 0.00202547],\n",
       "       [0.00220725, 0.0015741 , 0.00162604],\n",
       "       [0.00187593, 0.00115288, 0.00107148]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.z2_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Descent\n",
    "\n",
    "*Discussion:* Input to Hidden Weight Update\n",
    "- We multiply the gradient by the inputs. Why?\n",
    "- Why do we need to transpose the inputs? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00467941, 0.0031015 , 0.00296381],\n",
       "       [0.00557846, 0.00377897, 0.00364315]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T.dot(nn.z2_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discussion:* Hidden to Output Weight Update\n",
    "- Why is output the shape 3x1? \n",
    "- We multiply the gradient by the inputs. Why?\n",
    "- Why do we need to transpose the inputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01765649],\n",
       "       [0.01942327],\n",
       "       [0.02125417]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.activated_hidden.T.dot(nn.o_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Network (fo real this time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.61407722]\n",
      " [0.59988931]\n",
      " [0.61081424]]\n",
      "Loss: \n",
      " 0.07973033631912306\n",
      "+---------EPOCH 2---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.67974873]\n",
      " [0.65830891]\n",
      " [0.67603656]]\n",
      "Loss: \n",
      " 0.04806010607427904\n",
      "+---------EPOCH 3---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.72363946]\n",
      " [0.69801363]\n",
      " [0.71975666]]\n",
      "Loss: \n",
      " 0.03125994716814582\n",
      "+---------EPOCH 4---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.75426364]\n",
      " [0.7261539 ]\n",
      " [0.75033633]]\n",
      "Loss: \n",
      " 0.021629753511394328\n",
      "+---------EPOCH 5---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.7766533 ]\n",
      " [0.74701326]\n",
      " [0.77273752]]\n",
      "Loss: \n",
      " 0.01568825603785924\n",
      "+---------EPOCH 1000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.90170265]\n",
      " [0.87019431]\n",
      " [0.8971617 ]]\n",
      "Loss: \n",
      " 0.00016333571467972655\n",
      "+---------EPOCH 2000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.90227342]\n",
      " [0.87027723]\n",
      " [0.8964226 ]]\n",
      "Loss: \n",
      " 0.0001537010384085572\n",
      "+---------EPOCH 3000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.90273291]\n",
      " [0.87037926]\n",
      " [0.89579816]]\n",
      "Loss: \n",
      " 0.00014650004530841713\n",
      "+---------EPOCH 4000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.90310983]\n",
      " [0.87048836]\n",
      " [0.89526457]]\n",
      "Loss: \n",
      " 0.0001409997860694683\n",
      "+---------EPOCH 5000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.90342355]\n",
      " [0.87059776]\n",
      " [0.89480473]]\n",
      "Loss: \n",
      " 0.0001367255200241319\n",
      "+---------EPOCH 6000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.90368771]\n",
      " [0.87070368]\n",
      " [0.89440585]]\n",
      "Loss: \n",
      " 0.00013335710082507517\n",
      "+---------EPOCH 7000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.90391217]\n",
      " [0.87080409]\n",
      " [0.89405805]]\n",
      "Loss: \n",
      " 0.0001306714928437448\n",
      "+---------EPOCH 8000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.90410434]\n",
      " [0.87089801]\n",
      " [0.8937535 ]]\n",
      "Loss: \n",
      " 0.00012850912798409325\n",
      "+---------EPOCH 9000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.90426989]\n",
      " [0.8709851 ]\n",
      " [0.89348582]]\n",
      "Loss: \n",
      " 0.0001267533083780934\n",
      "+---------EPOCH 10000---------+\n",
      "Input: \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual Output: \n",
      " [[0.92]\n",
      " [0.86]\n",
      " [0.89]]\n",
      "Predicted Output: \n",
      " [[0.90441324]\n",
      " [0.8710654 ]\n",
      " [0.8932498 ]]\n",
      "Loss: \n",
      " 0.00012531710407457972\n"
     ]
    }
   ],
   "source": [
    "# Train my 'net\n",
    "nn = NeuralNetwork()\n",
    "\n",
    "# Number of Epochs / Iterations\n",
    "for i in range(10000):\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 1000 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', X)\n",
    "        print('Actual Output: \\n', y)\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
    "    nn.train(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "In the module project, you will implement backpropagation inside a multi-layer perceptron (aka a feedforward neural network). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The What - Stochastic Gradient Descent calculates an approximation of the gradient over the entire dataset by reviewing the predictions of a random sample. \n",
    "\n",
    "The Why - *Speed*. Calculating the gradient over the entire dataset is extremely expensive computationally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZF7UE-KluPsX"
   },
   "source": [
    "## Follow Along\n",
    "\n",
    "A true Stochastic GD-based implementation from [Welch Labs](https://www.youtube.com/watch?v=bxe2T-V8XRs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Neural_Network(object):\n",
    "    def __init__(self):        \n",
    "        #Define Hyperparameters\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        #Weights (parameters)\n",
    "        self.W1 = np.random.randn(self.inputLayerSize,self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #Propogate inputs though network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.sigmoid(self.z3) \n",
    "        return yHat\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        #Apply sigmoid activation function to scalar, vector, or matrix\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoidPrime(self,z):\n",
    "        #Gradient of sigmoid, la\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        #Compute cost for given X,y, use weights already stored in class.\n",
    "        self.yHat = self.forward(X)\n",
    "        J = 0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "        \n",
    "    def costFunctionPrime(self, X, y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yHat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yHat), self.sigmoidPrime(self.z3))\n",
    "        print(\"delta3\", delta3.shape)\n",
    "        dJdW2 = np.dot(self.a2.T, delta3)\n",
    "        print(\"dJdW2\", dJdW2.shape)\n",
    "        \n",
    "        delta2 = np.dot(delta3, self.W2.T)*self.sigmoidPrime(self.z2)\n",
    "        print(\"delta2\", delta2.shape)\n",
    "        dJdW1 = np.dot(X.T, delta2)  \n",
    "        print(\"dJdW1\", dJdW1.shape)\n",
    "        \n",
    "        return dJdW1, dJdW2\n",
    "    \n",
    "    #Helper Functions for interacting with other classes:\n",
    "    def getParams(self):\n",
    "        #Get W1 and W2 unrolled into vector:\n",
    "        params = np.concatenate((self.W1.ravel(), self.W2.ravel()))\n",
    "        return params\n",
    "    \n",
    "    def setParams(self, params):\n",
    "        #Set W1 and W2 using single paramater vector.\n",
    "        W1_start = 0\n",
    "        W1_end = self.hiddenLayerSize * self.inputLayerSize\n",
    "        self.W1 = np.reshape(params[W1_start:W1_end], (self.inputLayerSize , self.hiddenLayerSize))\n",
    "        W2_end = W1_end + self.hiddenLayerSize*self.outputLayerSize\n",
    "        self.W2 = np.reshape(params[W1_end:W2_end], (self.hiddenLayerSize, self.outputLayerSize))\n",
    "        \n",
    "    def computeGradients(self, X, y):\n",
    "        dJdW1, dJdW2 = self.costFunctionPrime(X, y)\n",
    "        return np.concatenate((dJdW1.ravel(), dJdW2.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uA9LaTgKr6rP"
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "class trainer(object):\n",
    "    def __init__(self, N):\n",
    "        #Make Local reference to network:\n",
    "        self.N = N\n",
    "        \n",
    "    def callbackF(self, params):\n",
    "        self.N.setParams(params)\n",
    "        self.J.append(self.N.costFunction(self.X, self.y))   \n",
    "        \n",
    "    def costFunctionWrapper(self, params, X, y):\n",
    "        self.N.setParams(params)\n",
    "        cost = self.N.costFunction(X, y)\n",
    "        grad = self.N.computeGradients(X,y)\n",
    "        \n",
    "        print(\"cost shape\", cost.shape)\n",
    "        print(\"grad shape\", grad.shape)\n",
    "        \n",
    "        return cost, grad\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        #Make an internal variable for the callback function:\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        #Make empty list to store costs:\n",
    "        self.J = []\n",
    "        \n",
    "        params0 = self.N.getParams()\n",
    "        print(params0.shape)\n",
    "\n",
    "        options = {'maxiter': 200, 'disp' : True}\n",
    "        _res = optimize.minimize(self.costFunctionWrapper, params0, jac=True, method='BFGS', \\\n",
    "                                 args=(X, y), options=options, callback=self.callbackF)\n",
    "\n",
    "        self.N.setParams(_res.x)\n",
    "        self.optimizationResults = _res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g_kHb6Se1u9y"
   },
   "outputs": [],
   "source": [
    "NN = Neural_Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hYYVhFf4rn3q"
   },
   "outputs": [],
   "source": [
    "T = trainer(NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 2), (3, 1))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "L-gYdVfgrysE",
    "outputId": "ae371bf9-692c-49b4-b165-8562dab9c06e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "delta3 (3, 1)\n",
      "dJdW2 (3, 1)\n",
      "delta2 (3, 3)\n",
      "dJdW1 (2, 3)\n",
      "cost shape (1,)\n",
      "grad shape (9,)\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 43\n",
      "         Function evaluations: 46\n",
      "         Gradient evaluations: 46\n"
     ]
    }
   ],
   "source": [
    "T.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "Jyv_L8Z2sKOA",
    "outputId": "08725651-6d21-401b-85c0-3487370b8bc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output: \n",
      "[[0.91983978]\n",
      " [0.86032317]\n",
      " [0.88989487]]\n",
      "Loss: \n",
      "4.7054600804689815e-08\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted Output: \\n\" + str(NN.forward(X))) \n",
    "print(\"Loss: \\n\" + str(np.mean(np.square(y - NN.forward(X))))) # mean sum squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "colab_type": "code",
    "id": "Gtf9WI9FtGPk",
    "outputId": "d062b2a3-5a92-403e-8ce0-c070aa79907b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAacUlEQVR4nO3dfZBcV33m8e8z3eqWpkf4TVOsY9mWWASOCWCTsYmBeEliG5HyWoQCbCewTsW7Cim8eSEkJYdas+v8sSZOEUjiyloVtGwIwcbmbQoUHGObLBViW+NXLHuFZWEsKYCEZfwiWS8z89s/7umZOz09My1prrpH9/mUu6bvuff2/EZq65lzz+1zFBGYmZm16ut2AWZm1pscEGZm1pYDwszM2nJAmJlZWw4IMzNrq9rtAubLsmXLYsWKFd0uw8xsQXnggQd+EhGD7fYdNwGxYsUKRkZGul2GmdmCIukHM+3zJSYzM2vLAWFmZm05IMzMrC0HhJmZteWAMDOzthwQZmbWlgPCzMzaKn1AvLj/EJ+483s89Mxz3S7FzKynlD4gRseCv7zrSR7e/tNul2Jm1lNKHxCNevZh8r0HRrtciZlZbyl9QNSqfSyqiL0Hx7pdiplZTyl9QAD016rscw/CzGwKBwTQqFV46YB7EGZmeQ4IsnGIfQfdgzAzy3NAAP31qscgzMxaOCDILjF5DMLMbKpCA0LSaklbJG2VtK7N/g9LelzSo5LuknRmbt+YpIfTY7jIOvtrVV5yQJiZTVHYinKSKsBNwMXADmCTpOGIeDx32EPAUETsk/Q7wJ8Bl6d9L0fEOUXVlzdQr7DPl5jMzKYosgdxPrA1IrZFxEHgFmBN/oCIuCci9qXNe4HlBdYzo34PUpuZTVNkQJwGbM9t70htM7ka+Mfc9mJJI5LulfSudidIWpuOGdm9e/cRF9qoVdjr21zNzKYo7BLT4ZD0fmAI+A+55jMjYqekVwF3S/puRDyVPy8i1gPrAYaGhuJIv39/rcrLh8YYGw8qfTrSlzEzO64U2YPYCZye216e2qaQdBHwUeCyiDjQbI+InenrNuBbwLlFFTqQ5mPyZSYzs0lFBsQmYJWklZJqwBXAlLuRJJ0L3EwWDrty7SdJqqfny4C3AvnB7XnVX68AeKDazCynsEtMETEq6RrgDqACbIiIzZKuB0YiYhi4ERgAbpME8ExEXAb8LHCzpHGyELuh5e6nedWoeUZXM7NWhY5BRMRGYGNL23W55xfNcN53gNcXWVve5JTf7kGYmTX5k9RkdzEB7PUYhJnZBAcE2ecgwIPUZmZ5DggmexCe8tvMbJIDgskxCE/YZ2Y2yQFB7i4m3+ZqZjbBAQEsSZeY3IMwM5vkgABq1T5qlT5e8iC1mdkEB0TSqFfY50FqM7MJDoikv1b15yDMzHIcEIl7EGZmUzkgkkbdPQgzszwHRNKoVT1Zn5lZjgMi6a95XWozszwHROJLTGZmUzkgkkbd61KbmeU5IBKPQZiZTeWASPprVQ6MjjM6Nt7tUszMeoIDImnUm4sG+TKTmRk4ICY0vGiQmdkUDoikv7nsqAeqzcwAB8SE5poQ7kGYmWUcEEl/vbnsqAPCzAwcEBMGJpYd9SUmMzNwQEzon1h21D0IMzNwQExo3ubq+ZjMzDIOiKR5m6s/TW1mlnFAJP2LfJurmVmeAyKpVvqoV/t8m6uZWVJoQEhaLWmLpK2S1rXZ/2FJj0t6VNJdks7M7btK0pPpcVWRdTY16lXf5mpmlhQWEJIqwE3AO4GzgSslnd1y2EPAUES8Abgd+LN07snAx4A3A+cDH5N0UlG1NjXqXjTIzKypyB7E+cDWiNgWEQeBW4A1+QMi4p6I2Jc27wWWp+fvAO6MiD0R8RxwJ7C6wFoBT/ltZpZXZECcBmzPbe9IbTO5GvjHwzlX0lpJI5JGdu/efZTletlRM7O8nhiklvR+YAi48XDOi4j1ETEUEUODg4NHXYfHIMzMJhUZEDuB03Pby1PbFJIuAj4KXBYRBw7n3PnWqFV9F5OZWVJkQGwCVklaKakGXAEM5w+QdC5wM1k47MrtugO4RNJJaXD6ktRWqH6vS21mNqFa1AtHxKika8j+Ya8AGyJis6TrgZGIGCa7pDQA3CYJ4JmIuCwi9kj6U7KQAbg+IvYUVWuTexBmZpMKCwiAiNgIbGxpuy73/KJZzt0AbCiuuuncgzAzm9QTg9S9YqBW5eDYOAdHx7tdiplZ1zkgcvrThH0v+1ZXMzMHRF6juS61xyHMzBwQeZ7y28xskgMip7lo0F5fYjIzc0DkNZcd3ecehJmZAyKvkQLC022YmTkgpvC61GZmkxwQOROD1L6LyczMAZHXn25z3edPU5uZOSDy+j0GYWY2wQGRU+kTSxZVPGGfmRkOiGka9Yo/B2FmhgNimv5a1Z+DMDPDATFNtuyoexBmZg6IFo2axyDMzMABMU1/veoxCDMzHBDTNGoVj0GYmeGAmKZRr3q6bzMzHBDTNGq+zdXMDBwQ0/TXqx6kNjPDATFNo1bh0FhwYNS9CDMrNwdEi+aMrp6wz8zKzgHRorlokKf8NrOyc0C06PeiQWZmgANiGi87amaWcUC08BiEmVnGAdGiuaqcxyDMrOwcEC0mehAOCDMruUIDQtJqSVskbZW0rs3+CyU9KGlU0nta9o1Jejg9housM6+RBqk95beZlV21qBeWVAFuAi4GdgCbJA1HxOO5w54BfhP4SJuXeDkizimqvpk0B6k9YZ+ZlV1hAQGcD2yNiG0Akm4B1gATARERT6d94wXWcViWLGqOQbgHYWblVuQlptOA7bntHamtU4sljUi6V9K72h0gaW06ZmT37t1HU+uEvj7RX6t4RlczK71eHqQ+MyKGgF8HPinp37ceEBHrI2IoIoYGBwfn7Rs3PGGfmVmhAbETOD23vTy1dSQidqav24BvAefOZ3GzadQq7PUgtZmVXJEBsQlYJWmlpBpwBdDR3UiSTpJUT8+XAW8lN3ZRtP6aexBmZoUFRESMAtcAdwBPAF+IiM2Srpd0GYCk8yTtAN4L3Cxpczr9Z4ERSY8A9wA3tNz9VKhGveKpNsys9Dq6i0nSZyPiA3O1tYqIjcDGlrbrcs83kV16aj3vO8DrO6mtCI16lT17D3br25uZ9YROexCvy2+kzzj8/PyX0xsaNa9LbWY2a0BIulbSi8AbJL2QHi8Cu4CvHpMKu6C/VvF032ZWerMGRET8z4hYCtwYEa9Ij6URcUpEXHuMajzmGvWqxyDMrPQ6vcT0NUkNAEnvl/QJSWcWWFdXNepZDyIiul2KmVnXdBoQfwPsk/RG4A+Bp4C/K6yqLuuvVRkbDw6M9swMIGZmx1ynATEa2a/Ta4C/joibgKXFldVdjeaaEL7MZGYl1mlAvCjpWuADwNcl9QGLiiuruybXhPBAtZmVV6cBcTlwAPitiPgR2WcXbiysqi4bqHtdajOzjgIihcLngBMkXQrsj4jjdwwiBYQvMZlZmXUUEJLeB9xPNiXG+4D7WleAO54MTKwq54Aws/LqdMGgjwLnRcQuAEmDwDeB24sqrJsaEz0Ij0GYWXl1OgbR1wyH5NnDOHfBaS476ktMZlZmnfYgviHpDuDzaftyWibhO554kNrMbI6AkPRq4JUR8UeS3g28Le36V7JB6+NSw4PUZmZz9iA+CVwLEBFfAr4EIOn1ad9/LLS6LqlV+6hV+njJiwaZWYnNNY7wyoj4bmtjaltRSEU9olGvuAdhZqU2V0CcOMu+JfNZSK9p1Ku+i8nMSm2ugBiR9F9aGyX9Z+CBYkrqDQOe8tvMSm6uMYjfB74s6TeYDIQhoAb8WpGFdVvWg3BAmFl5zRoQEfFj4C2Sfgn4udT89Yi4u/DKuqxRr/L8Pq9LbWbl1dHnICLiHuCegmvpKQP1Cjufcw/CzMrruP009NFq1Kqe7tvMSs0BMQOvS21mZeeAmMFAGqT2utRmVlYOiBk06lXGA/Yf8rrUZlZODogZeE0IMys7B8QMPGGfmZWdA2IG/TVP+W1m5VZoQEhaLWmLpK2S1rXZf6GkByWNti5hKukqSU+mx1VF1tnOgHsQZlZyhQWEpApwE/BO4GzgSklntxz2DPCbwD+0nHsy8DHgzcD5wMcknVRUre000hjEXk/5bWYlVWQP4nxga0Rsi4iDwC3AmvwBEfF0RDwKtN4q9A7gzojYExHPAXcCqwusdZrJVeX8YTkzK6ciA+I0YHtue0dqm7dzJa2VNCJpZPfu3UdcaDsepDazslvQg9QRsT4ihiJiaHBwcF5f2wFhZmVXZEDsBE7PbS9PbUWfOy8aNX8OwszKrciA2ASskrRSUg24Ahju8Nw7gEsknZQGpy9JbcdMtdLH4kV97kGYWWkVFhARMQpcQ/YP+xPAFyJis6TrJV0GIOk8STuA9wI3S9qczt0D/ClZyGwCrk9tx1S2qpwHqc2snDpaD+JIRcRGYGNL23W555vILh+1O3cDsKHI+ubSqFfZ59tczaykFvQgddEaNS87ambl5YCYxYDXhDCzEnNAzKJRr7DXYxBmVlIOiFk06r7EZGbl5YCYhS8xmVmZOSBm4R6EmZWZA2IWjXqVvQfHGB/3utRmVj4OiFk0lx3dd8gD1WZWPg6IWTRXlfNlJjMrIwfELCbXhHBAmFn5OCBm4Sm/zazMHBCzaC476h6EmZWRA2IWAxM9CA9Sm1n5OCBm4UtMZlZmDohZeJDazMrMATGLZg/Ca0KYWRk5IGbRv6g5SO0xCDMrHwfELPr6RKNW8RiEmZWSA2IOnrDPzMrKATEHT/ltZmXlgJiDexBmVlYOiDl42VEzKysHxBx8icnMysoBMYds0SAHhJmVjwNiDh6DMLOyckDMwZeYzKysHBBzaNSq7D80zujYeLdLMTM7phwQc2iuCbH3oO9kMrNyKTQgJK2WtEXSVknr2uyvS7o17b9P0orUvkLSy5IeTo//VWSds/GU32ZWVtWiXlhSBbgJuBjYAWySNBwRj+cOuxp4LiJeLekK4OPA5WnfUxFxTlH1dcoBYWZlVWQP4nxga0Rsi4iDwC3AmpZj1gD/Jz2/HfgVSSqwpsM24EtMZlZSRQbEacD23PaO1Nb2mIgYBZ4HTkn7Vkp6SNI/S/rFAuucVaPmHoSZlVNhl5iO0g+BMyLiWUk/D3xF0usi4oX8QZLWAmsBzjjjjEIKaXhVOTMrqSJ7EDuB03Pby1Nb22MkVYETgGcj4kBEPAsQEQ8ATwGvaf0GEbE+IoYiYmhwcLCAH2Fy2VH3IMysbIoMiE3AKkkrJdWAK4DhlmOGgavS8/cAd0dESBpMg9xIehWwCthWYK0z8iC1mZVVYZeYImJU0jXAHUAF2BARmyVdD4xExDDwaeCzkrYCe8hCBOBC4HpJh4Bx4IMRsaeoWmczMHGJyYPUZlYuhY5BRMRGYGNL23W55/uB97Y574vAF4usrVOLF/XRJ/cgzKx8/EnqOUii4fmYzKyEHBAdGPCMrmZWQg6IDnhNCDMrIwdEB7JLTB6kNrNycUB0YKBe8SUmMysdB0QHGjWPQZhZ+TggOuBV5cysjBwQHfC61GZWRg6IDvTXK57u28xKxwHRgYFalYOj4xzyutRmViIOiA54wj4zKyMHRAcGvCaEmZWQA6IDkz0Ij0OYWXk4IDrQSOtSuwdhZmXigOiAV5UzszJyQHTAg9RmVkYOiA54kNrMysgB0QH3IMysjBwQHWgOUvvT1GZWJg6IDtSrFRZV5EtMZlYqDogOecI+MysbB0SHGjVP+W1m5eKA6NCAexBmVjIOiA416hVPtWFmpeKA6NDJjTqb/+15tu56sdulmJkdEw6IDv3RO15Lpa+P9918L4/tfL7b5ZiZFc4B0aHX/rul3PbBC1iyqMKV6+9l5Ok93S7JzKxQDojDsHJZg9s+eAGDS+t84NP38+0nd3e7JDOzwjggDtPPnLiEW3/7As48pZ+rPzPCX9z5PXa9sL/bZZmZzbtCA0LSaklbJG2VtK7N/rqkW9P++yStyO27NrVvkfSOIus8XINL69y69gIufM0gn7rrSd5yw9186B8e5L5tzxIR3S7PzGxeVIt6YUkV4CbgYmAHsEnScEQ8njvsauC5iHi1pCuAjwOXSzobuAJ4HfAzwDclvSYieuY+0xP6F/G3Vw3x/Z/s5e/v/QG3jWzn64/+kBWn9HPqCUtYurjKwOIqr1i8iMWLKkjQJxCiT4DU0ffp5Kh2L6V0ZnOfcscqNfYpqyWrTUiiIujrE30Slb7sUav0Ua2Ial8ftapYvKjCkkUVltSyr4sXVaj2ZfsrFVFN57fW1+5nkTSltmabmXVfYQEBnA9sjYhtAJJuAdYA+YBYA/z39Px24K+V/euwBrglIg4A35e0Nb3evxZY7xFZuazBf7v0bD5yyWsZfmQndz6+i+dfPsgze/bx4v5RXth/iAOHxgmCCBiPYNydjMPSC3kxnyU0A3BaMKKJRtEM7ez5RJDm9k+8TusxR1TTEZ7Y4Xdsff0e+Cs9zF+spv7CI009tvWl2v2So2lPOjuvbe0t22ed+gr+6spzOzr3cBQZEKcB23PbO4A3z3RMRIxKeh44JbXf23Luaa3fQNJaYC3AGWecMW+FH4kltQqXn3cGl593bOtod0mr2RQtx0Ta1wwrmAysiGB8PNsei2B8PPs6OhaMjgejY+McGgsOjo1z4NAYLx8a4+WD2df9h8YZG8/2j40Hh8bHJ2tofu82oRjka40px0w87YFLdvNZQf7nnbrdsi/7j/HxmPL3lj8nez59/5HWdNjnHfHr98Dfabv34yx/N83t/N9N1jb9T32m93rz+Na2mRtmqL3NgaeftKSzkw9TkQFRuIhYD6wHGBoa6v67rgva/qYyrakXfl8zs4WmyEHqncDpue3lqa3tMZKqwAnAsx2ea2ZmBSoyIDYBqyStlFQjG3QebjlmGLgqPX8PcHdkfbBh4Ip0l9NKYBVwf4G1mplZi8IuMaUxhWuAO4AKsCEiNku6HhiJiGHg08Bn0yD0HrIQIR33BbIB7VHgQ710B5OZWRnoeLlvf2hoKEZGRrpdhpnZgiLpgYgYarfPn6Q2M7O2HBBmZtaWA8LMzNpyQJiZWVvHzSC1pN3AD47iJZYBP5mnco6lhVo3uPZuce3d0au1nxkRg+12HDcBcbQkjcw0kt/LFmrd4Nq7xbV3x0Ks3ZeYzMysLQeEmZm15YCYtL7bBRyhhVo3uPZuce3dseBq9xiEmZm15R6EmZm15YAwM7O2Sh8QklZL2iJpq6R13a5nNpI2SNol6bFc28mS7pT0ZPp6UjdrnImk0yXdI+lxSZsl/V5q7/n6JS2WdL+kR1Lt/yO1r5R0X3rv3Jqmte85kiqSHpL0tbS9UOp+WtJ3JT0saSS19fz7BUDSiZJul/T/JD0h6YKFUnteqQNCUgW4CXgncDZwpaSzu1vVrD4DrG5pWwfcFRGrgLvSdi8aBf4wIs4GfgH4UPqzXgj1HwB+OSLeCJwDrJb0C8DHgb+IiFcDzwFXd7HG2fwe8ERue6HUDfBLEXFO7vMDC+H9AvAp4BsRcRbwRrI//4VS+6SIKO0DuAC4I7d9LXBtt+uao+YVwGO57S3Aqen5qcCWbtfY4c/xVeDihVY/0A88SLa++k+Aarv3Uq88yFZjvAv4ZeBrZOvP9nzdqbangWUtbT3/fiFbGfP7pJuAFlLtrY9S9yCA04Dtue0dqW0heWVE/DA9/xHwym4W0wlJK4BzgftYIPWnyzQPA7uAO4GngJ9GxGg6pFffO58E/hgYT9unsDDqBgjgnyQ9IGltalsI75eVwG7gf6dLe38rqcHCqH2KsgfEcSWyX016+r5lSQPAF4Hfj4gX8vt6uf6IGIuIc8h+Iz8fOKvLJc1J0qXAroh4oNu1HKG3RcSbyC4Bf0jShfmdPfx+qQJvAv4mIs4F9tJyOamHa5+i7AGxEzg9t708tS0kP5Z0KkD6uqvL9cxI0iKycPhcRHwpNS+Y+gEi4qfAPWSXZk6U1Fy2txffO28FLpP0NHAL2WWmT9H7dQMQETvT113Al8mCeSG8X3YAOyLivrR9O1lgLITapyh7QGwCVqW7Ompka2IPd7mmwzUMXJWeX0V2bb/nSBLZGuRPRMQncrt6vn5Jg5JOTM+XkI2dPEEWFO9Jh/Vc7RFxbUQsj4gVZO/tuyPiN+jxugEkNSQtbT4HLgEeYwG8XyLiR8B2Sa9NTb8CPM4CqH2abg+CdPsB/CrwPbJryh/tdj1z1Pp54IfAIbLfUq4mu6Z8F/Ak8E3g5G7XOUPtbyPrUj8KPJwev7oQ6gfeADyUan8MuC61vwq4H9gK3AbUu13rLD/D24GvLZS6U42PpMfm5v+bC+H9kuo8BxhJ75mvACctlNrzD0+1YWZmbZX9EpOZmc3AAWFmZm05IMzMrC0HhJmZteWAMDOzthwQZomkl9LXFZJ+fZ5f+09atr8zn69vVgQHhNl0K4DDCojcJ5NnMiUgIuIth1mT2THngDCb7gbgF9M6BH+QJuq7UdImSY9K+m0ASW+X9G1Jw2SflEXSV9LkcpubE8xJugFYkl7vc6mt2VtReu3H0toHl+de+1u5NQU+lz6NjqQb0roaj0r682P+p2OlMddvPWZltA74SERcCpD+oX8+Is6TVAf+RdI/pWPfBPxcRHw/bf9WROxJU3JskvTFiFgn6ZrIJvtr9W6yT92+EViWzvm/ad+5wOuAfwP+BXirpCeAXwPOiohoTgFiVgT3IMzmdgnwn9J03/eRTZmwKu27PxcOAL8r6RHgXrKJIFcxu7cBn49sttgfA/8MnJd77R0RMU42NckK4HlgP/BpSe8G9h31T2c2AweE2dwE/NfIVjY7JyJWRkSzB7F34iDp7cBFwAWRrT73ELD4KL7vgdzzMbJFfkbJZjW9HbgU+MZRvL7ZrBwQZtO9CCzNbd8B/E6arhxJr0kzjLY6AXguIvZJOotsadWmQ83zW3wbuDyNcwwCF5JNpNdWWk/jhIjYCPwB2aUps0J4DMJsukeBsXSp6DNkayisAB5MA8W7gXe1Oe8bwAfTOMEWsstMTeuBRyU9GNmU201fJltb4hGy2W7/OCJ+lAKmnaXAVyUtJuvZfPjIfkSzuXk2VzMza8uXmMzMrC0HhJmZteWAMDOzthwQZmbWlgPCzMzackCYmVlbDggzM2vr/wPwxdzLoUymigAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(T.J)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "This is a reference implementation for you to explore. You will not be expected to apply it to today's module project. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lambda",
   "language": "python",
   "name": "lambda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
